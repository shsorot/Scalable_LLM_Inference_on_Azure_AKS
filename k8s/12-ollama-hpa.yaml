# Horizontal Pod Autoscaler for Ollama
# Scales based on GPU memory utilization
# Requires: DCGM Exporter + Prometheus + Metrics Server
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ollama-hpa
  namespace: ollama
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: ollama

  # Scaling boundaries
  minReplicas: 1   # Start with 1 pod, maximize single-GPU utilization first
  maxReplicas: 5   # Scale up when GPU memory exhausted (triggers cluster autoscaler)

  # Scaling metrics
  metrics:
    # Primary: GPU memory utilization
    # Note: Requires Prometheus Adapter with custom metrics
    # Metric format: DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL * 100
    # Returns percentage as milli-units (e.g., 88.487 = 88487m)
    - type: Pods
      pods:
        metric:
          name: gpu_memory_utilization
        target:
          type: AverageValue
          averageValue: 60000m  # Scale when GPU memory > 60% (60000 milli-units - LOWERED FOR DEMO)

    # Fallback: CPU utilization (only scale if CPU is bottleneck)
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 85

    # Memory utilization (system RAM) - less critical for GPU workloads
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 85

  # Scaling behavior - conservative for GPU workloads
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 600  # Wait 10 min before scaling down (models in memory)
      policies:
        - type: Pods
          value: 1
          periodSeconds: 300  # Remove max 1 pod per 5 minutes
      selectPolicy: Min

    scaleUp:
      stabilizationWindowSeconds: 60  # Quick scale up for GPU demand
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60  # Add 1 pod per minute
      selectPolicy: Max
---
# NOTE: GPU memory metrics require additional setup:
# 1. DCGM Exporter (11-dcgm-exporter.yaml) - deploys GPU metrics
# 2. Prometheus - scrapes DCGM metrics
# 3. Prometheus Adapter - exposes custom metrics to HPA
#
# For initial demo without custom metrics, this HPA will use CPU/memory only.
# GPU metric will be ignored until Prometheus Adapter is configured.
#
# To enable GPU metrics:
# 1. Deploy Prometheus (via Helm): helm install prometheus prometheus-community/kube-prometheus-stack
# 2. Deploy Prometheus Adapter with custom rules for DCGM metrics
# 3. Verify metrics: kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq .
